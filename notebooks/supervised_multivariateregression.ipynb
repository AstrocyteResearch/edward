{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Supervised Multivariate Normal Learning (MVN Regression)\n",
    "\n",
    "In supervised learning, the task is to infer hidden structure from\n",
    "labeled data, comprised of training examples $\\{(x_n, y_n)\\}$.\n",
    "Regression typically means the output $Y$ takes continuous values.  \n",
    "$Y$ has $D$ dimensions and $X$ has $Q$ dimensions.\n",
    "\n",
    "This builds on the single variable example in: http://edwardlib.org/tutorials/supervised-regression\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is not available at the moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal, MultivariateNormalTriL\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data\n",
    "\n",
    "Simulate training and test sets of $100$ data points, $2$ $Y$ variables and $10$ $X$ factors (one of which is a constant value). \n",
    "\n",
    "They comprise of pairs of inputs $\\mathbf{x}_n\\in\\mathbb{R}^{10}$ and outputs\n",
    "$Y_n\\in\\mathbb{R}^{2}$. They have a linear dependence with normally distributed and correlated noise $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, output_shape, input_shape, true_weights, true_cov,  add_const = False):\n",
    "    \"\"\"\n",
    "    model: Y = XB + e  \n",
    "    where: Y is (N, output_shape), X is (N, input_shape), \n",
    "           B is (input_shape, output_shape) and e is (N, output_shape),\n",
    "           e_i ~ N(0, cov), cov is (output_shape, output_shape) and positive, semi-definite\n",
    "\n",
    "    if add_const: will set the last column of X to constant at the true_weights[-1] value\n",
    "    \"\"\"    \n",
    "    import numpy as np\n",
    "    \n",
    "    if output_shape > 1:\n",
    "        assert (output_shape, output_shape) == true_cov.shape, \"true_cov has wrong shape\"\n",
    "        assert (input_shape, output_shape) == true_weights.shape, \"true_weights has wrong shape\"\n",
    "    if add_const:\n",
    "        assert input_shape > 0, \"if add_const is true\"\n",
    "    \n",
    "    # Could just call errors = np.random.multivariate_normal(0, cov, N)    \n",
    "    L = np.linalg.cholesky(true_cov)\n",
    "    e = np.random.standard_normal((output_shape,N))\n",
    "    errors = np.dot(L,e).T\n",
    "    \n",
    "    # Create X\n",
    "    if add_const:\n",
    "        X = np.hstack([np.random.normal(0.0, 2.0, size=(N, input_shape-1)), np.ones((N,1))])\n",
    "    else:\n",
    "        X = np.random.normal(0.0, 2.0, size=(N, input_shape))\n",
    "    \n",
    "    # Calculate Y\n",
    "    Y = np.dot(X, true_weights) + errors\n",
    "    \n",
    "    return X, Y, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ed.set_seed(42)\n",
    "\n",
    "N = 1000  # number of data points\n",
    "D = 2    # number of target variables\n",
    "Q = 5   # number of features\n",
    "\n",
    "true_weights = np.random.randn(Q,D) * 10\n",
    "true_cov = np.array([[1,0.75],[0.75 , 2]])\n",
    "X_train, Y_train, Errors_train = build_toy_dataset(N, D, Q, true_weights, true_cov, add_const = True)\n",
    "X_test, Y_test, Errors_test = build_toy_dataset(N, D, Q, true_weights, true_cov, add_const = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.pairplot(pd.DataFrame(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model\n",
    "\n",
    "Posit the model as Bayesian linear regression (Murphy, 2012).\n",
    "It assumes a linear relationship between the inputs\n",
    "$\\mathbf{x}\\in\\mathbb{R}^Q$ and the outputs $Y\\in\\mathbb{R}^D$.\n",
    "\n",
    "For a set of $N$ data points $(\\mathbf{X},\\mathbf{Y})=\\{(\\mathbf{x}_n, y_n)\\}$,\n",
    "the model posits the following distributions:\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{B})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{\\mu_B} \\mid \\mathbf{0}, \\sigma_B^2\\mathbf{I}),\n",
    "  \\\\[1.5ex]\n",
    "  p(\\mathbf{\\Sigma_Y})\n",
    "  &=\n",
    "  \\text{InverseWishart}(S, \\nu),\n",
    "  \\\\[1.5ex]\n",
    "  p(\\mathbf{Y} \\mid \\mathbf{B}, \\mathbf{X}, \\mathbf{\\Sigma_Y})\n",
    "  &=\n",
    "  \\prod_{n=1}^N\n",
    "  \\text{Normal}(Y_n \\mid \\mathbf{x}_n^\\top\\mathbf{B}, \\Sigma_Y).\n",
    "\\end{align*}\n",
    "\n",
    "The latent variables are the linear model's weights $\\mathbf{B}$\n",
    "Assume $\\sigma_B^2$ is a known prior variances and $\\Sigma_Y$ is a\n",
    "known likelihood variance. \n",
    "\n",
    "The mean of the likelihood is given by a\n",
    "linear transformation of the inputs $\\mathbf{x}_n$.\n",
    "\n",
    "Let's build the model in Edward, fixing $\\mu_B=0$, $\\sigma_B=1$ and $\\Sigma_Y$ to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Single Variable Code\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [N, Q], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, [N, D], name=\"Y\")\n",
    "\n",
    "# Beta Priors\n",
    "beta_loc = tf.zeros((Q, D), dtype=tf.float32, name=\"Beta_loc\")\n",
    "beta_scale = tf.ones((Q, D), dtype=tf.float32, name=\"Beta_scale\")\n",
    "B = Normal(loc=beta_loc, scale=beta_scale, name=\"Beta\")\n",
    "\n",
    "# Covar Priors\n",
    "# Create Lower Triangular Prior for MultivariateNormalTriL\n",
    "scale = tf.cholesky(tf.convert_to_tensor(true_cov, tf.float32), name=\"Y_scale_tril\")\n",
    "mvn_scale = tf.tile(tf.reshape(scale, [1,2,2]), [N,1,1], name=\"Y_scale_tril_tiled\")\n",
    "\n",
    "# Model multivariate normal (do I add sample_shape=N to this?)\n",
    "\n",
    "Y = MultivariateNormalTriL(loc = tf.matmul(X,B), scale_tril=mvn_scale, name = \"Y_model\")\n",
    "#Y = MultivariateNormalTriL(loc = ed.dot(X, B), scale_tril=mvn_scale, name = \"Y_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we define a placeholder `X`. During inference, we pass in\n",
    "the value for this placeholder according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inference\n",
    "\n",
    "We now turn to inferring the posterior using variational inference.\n",
    "Define the variational model to be a fully factorized normal across\n",
    "the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Posterior Variables\n",
    "qB_loc = tf.Variable(tf.random_normal([Q,D], dtype=tf.float32), name='B_posterior_loc')\n",
    "#qb_scale_normal = tf.Variable(tf.random_normal([Q,D]), dtype=tf.float32, name='B_posterior_scale_normal' )\n",
    "#qB_scale = tf.nn.softplus(qb_scale_normal, name='B_posterior_scale')\n",
    "qB_scale = tf.ones((Q, D), dtype=tf.float32, name=\"Beta_scale\") / 100.0\n",
    "#qB = Normal(loc = qB_loc, scale = qB_scale, name = \"B_posterior\")\n",
    "qB = Normal(loc = qB_loc, scale = qB_scale, name = \"B_posterior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run variational inference with the Kullback-Leibler divergence, using \n",
    "$250$ iterations and $5$ latent variable samples in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "logdir_base = 'C:\\\\Users\\skruz\\\\temp\\\\tensorflow\\\\MLR\\\\'\n",
    "log_run = datetime.datetime.strftime(datetime.datetime.utcnow(), \"%Y-%m-%dT%H%M%SZ\")\n",
    "logdir = os.path.join(logdir_base, log_run)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inference = ed.KLqp({B: qB}, data={X: X_train, Y: Y_train})\n",
    "#inference = ed.KLqp({B: qB}, data={X: X_train, Y: Y_train})\n",
    "#inference.run(n_samples=5, n_iter=250, logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference = ed.KLqp({B: qB}, data={X: X_train, Y: Y_train})\n",
    "\n",
    "inference.initialize(n_samples=5, n_iter=1000, logdir=logdir)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "results = []\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  info_dict = inference.update()\n",
    "  inference.print_progress(info_dict)\n",
    "  results.append(info_dict)\n",
    "\n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(results).set_index('t')['loss'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit using normal linear regression\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "m = LinearRegression(fit_intercept=False)\n",
    "m.fit(X=X_train, y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as  sns\n",
    "print(\"This is absolute deviations from targets\")\n",
    "sns.heatmap(np.abs((true_weights.T - m.coef_) / true_weights.std()), vmin=0.0, vmax=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the model converge to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_betas = qB.mean().eval()\n",
    "print(\"This is the fitted absolute deviations from targets\")\n",
    "sns.heatmap(np.abs((true_weights.T - mean_betas.T) / true_weights.std()), vmin=0.0, vmax=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_dists(w, qw, actual=None, n_samples=1000, figsize=None):\n",
    "    \"\"\"\n",
    "    Creates a grid of histograms of samples from two distributions 'w' and 'qw'\n",
    "    Optionally creates a red vertical line for each value in 'acutal'\n",
    "        \n",
    "    Args:\n",
    "        w:  the prior variable with an ability to sample  \n",
    "        qw:  the posterior variable with the same dimensions as w\n",
    "        actual:  a list-like of known true values, has the same dimension as w\n",
    "        n_samples (int): the number of samples, default = 1000 \n",
    "        figsize (tuple): a tuple of the figure size, default is (20, 2*D) where D is the dimension of w \n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure: A gr\n",
    "        \n",
    "    Usage:\n",
    "        visualize_dists(w, qw, actual=[0.1, 0.0, 0.23, 0.5, 0.3], n_samples=1000)\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pylab as plt\n",
    "    \n",
    "    w_samples = w.sample(n_samples).eval()\n",
    "    qw_samples = qw.sample(n_samples).eval()\n",
    "    \n",
    "    w_samples = np.reshape(w_samples, [n_samples, np.product(w_samples.shape[1:])])\n",
    "    qw_samples = np.reshape(qw_samples, [n_samples, np.product(qw_samples.shape[1:])])\n",
    "    \n",
    "    actual = actual.flatten()\n",
    "\n",
    "    D = w_samples.shape[1]\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (20, D * 2)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    axes = []\n",
    "    for i in range(D):\n",
    "        if i == 0:\n",
    "            ax = fig.add_subplot(np.ceil(D / 2), 2, i + 1)\n",
    "        else:\n",
    "            ax = fig.add_subplot(np.ceil(D / 2), 2, i + 1)\n",
    "        sns.distplot(w_samples[:, i], kde=False, rug=False, ax=ax, color='grey')\n",
    "        sns.distplot(qw_samples[:, i], kde=False, rug=False, ax=ax, color='b')\n",
    "        ax.set_title('variable {}'.format(i))\n",
    "        axes.append(ax)\n",
    "\n",
    "    if not actual is None:\n",
    "        for i in range(D):\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            axes[i].vlines([actual[i]], ymin=ymin, ymax=ymax, color='r')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('prior vs posterier weight samples')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = visualize_dists(B, qB, actual=true_weights, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.get_all_collection_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = graph.get_collection('variables')[0]\n",
    "print(n.name)\n",
    "graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/0\n",
    "inference = ed.KLqp({B: qB}, data={X: X_train, Y: Y_train})\n",
    "\n",
    "results = []\n",
    "for optimizer in ['gradientdescent', 'adadelta','adagrad','momentum', 'adam','ftrl','rmsprop']:\n",
    "    print(\"optimizer: {}\".format(optimizer))\n",
    "    inference.initialize(n_samples=5, n_iter=100, logdir=logdir, optimizer = optimizer, var_list=[qB_loc,qB_scale])\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for _ in range(inference.n_iter):\n",
    "      info_dict = inference.update()\n",
    "      info_dict['optimizer'] = optimizer\n",
    "      results.append(info_dict)\n",
    "      inference.print_progress(info_dict)\n",
    "\n",
    "    inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Check trainable_variables should only have var_list=[qB_loc,qB_scale]\n",
    "# TODO: Solve NotImplementedError: ('Trying to optimize unsupported type ', <tf.Tensor B_posterior_scale':0' shape=(5, 2) dtype=float32\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma = ed.models.Gamma(10.0,100.0)\n",
    "data = ed.models.Normal(loc=0.0, scale=sigma)\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "N=10000\n",
    "with sess.as_default():\n",
    "    x_train = data.sample(N).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma = ed.models.Gamma(1.0,1.0)\n",
    "x = ed.models.Normal(loc=tf.zeros([N]), scale=sigma*tf.ones([N]))\n",
    "\n",
    "#qsigma = ed.models.GammaWithSoftplusConcentrationRate(tf.Variable(1.0), tf.Variable(1.0))\n",
    "\n",
    "#from edward.models import TransformedDistribution\n",
    "ds = tf.contrib.distributions\n",
    "qlognormal = ds.TransformedDistribution(\n",
    "    distribution=ds.Normal(loc=tf.Variable(0.0), scale=tf.Variable(1.0)),\n",
    "    bijector=ds.bijectors.Inline(\n",
    "      forward_fn=tf.exp,\n",
    "      inverse_fn=tf.log,\n",
    "      inverse_log_det_jacobian_fn=(\n",
    "        lambda y: -tf.reduce_sum(tf.log(y), axis=-1)),\n",
    "    name=\"LogNormalTransformedDistribution\"))\n",
    "\n",
    "\n",
    "# INFERENCE\n",
    "qlognormal = TransformedDistribution(base_dist_cls=Normal, mu=tf.zeros(1), sigma=tf.ones(1), transform=tf.log,\n",
    "  inverse=tf.exp)\n",
    "\n",
    "\n",
    "inference = ed.KLqp({sigma: qlognormal},data={x: x_train})\n",
    "inference.run(n_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[qsigma.parameters['concentration'].eval(),qsigma.parameters['rate'].eval()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qx=ed.copy(x,{sigma:qsigma})\n",
    "plt.hist(x_train,alpha=0.5, color='grey')\n",
    "plt.hist(qx.sample().eval(), color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference = ed.Inference({B: qB}, data={X: X_train, Y: Y_train})\n",
    "inference.run(n_samples=5, n_iter=250, logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference = ed.KLqp({B: qB}, data={X: X_train, Y: Y_train})\n",
    "inference.initialize()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  info_dict = inference.update()\n",
    "  inference.print_progress(info_dict)\n",
    "\n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this case `KLqp` defaults to minimizing the\n",
    "$\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "gradient.\n",
    "For more details on inference, see the [$\\text{KL}(q\\|p)$ tutorial](http://edwardlib.org/tutorials/klqp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Criticism\n",
    "\n",
    "A standard evaluation for regression is to compare prediction accuracy on\n",
    "held-out \"testing\" data. We do this by first forming the posterior predictive\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_post = ed.copy(Y, {B: qB})\n",
    "# This is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(X, qw) + qb, scale=tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With this we can evaluate various quantities using predictions from\n",
    "the model (posterior predictive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={X: X_test, Y_post: Y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={X: X_test, Y_post: Y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The trained model makes predictions with low error\n",
    "(relative to the magnitude of the output).\n",
    "\n",
    "We can also visualize the fit by comparing data generated with the\n",
    "prior to data generated with the posterior (on the first feature\n",
    "dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Y_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.summary.get_summary_description(Y_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_samples = Y.sample(50).eval()\n",
    "Y_post_samples = Y_post.sample(50).eval()\n",
    "t = np.random.randint(0,N)\n",
    "for i in range(D):\n",
    "    fig = plt.figure(); ax=fig.add_subplot(111)\n",
    "    pd.DataFrame(Y_samples[:,t,i],columns=['Y_{}'.format(i)]).hist(ax=ax, color='grey')\n",
    "    pd.DataFrame(Y_post_samples[:,t,i],columns=['Y_{}'.format(i)]).hist(ax=ax, color='blue')\n",
    "    lims = ax.get_ylim()\n",
    "    ax.vlines([Y_train[t,i]], ymin=lims[0], ymax=lims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B_samples = B.sample(50).eval()\n",
    "B_post_samples = qB.sample(50).eval()\n",
    "print(B_samples.shape, B_post_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.random.randint(0,N)\n",
    "fig = plt.figure();\n",
    "axes = []\n",
    "index = 0\n",
    "for i in range(D):\n",
    "    for j in range(Q):\n",
    "        axes.append(fig.add_subplot(D,Q,index+1))\n",
    "        pd.DataFrame(B_samples[:,j,i],columns=['Y_{}'.format(i)]).hist(ax=axes[-1], color='grey')\n",
    "        pd.DataFrame(B_post_samples[:,j,i],columns=['Y_{}'.format(i)]).hist(ax=axes[-1], color='blue')\n",
    "        index += 1\n",
    "index = 0\n",
    "for i in range(D):\n",
    "    for j in range(Q):\n",
    "        lims = ax.get_ylim()\n",
    "        axes[index].vlines([true_weights[j,i]], ymin=lims[0], ymax=lims[1], color='r')\n",
    "        index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Edward Inference Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sess = ed.get_session()\n",
    "#file_writer = tf.summary.FileWriter('C:\\\\Users\\skruz\\\\temp\\\\tensorflow\\\\', sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "Launch Tensorboard by calling\n",
    "    > python -m tensorflow.tensorboard --logdir=C:\\Users\\skruz\\temp\\tensorflow --host=localhost\n",
    "    \n",
    "Open URL: http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = qB_loc.initialized_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
